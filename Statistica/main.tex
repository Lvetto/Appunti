\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{geometry}
 \geometry{
 a4paper,
 total={190mm,257mm},
 left=10mm,
 top=20mm,
 }

\title{Statistica}
\author{Luca Vettore}
\date{$1^o$ semestre 2021-2022}
\begin{document}
\maketitle

\section{Le incertezze}
Ogni misura è affetta da \textbf{errori}. Questi errori possono essere dovuti a fattori diversi, ma possono essere raggruppati in \textbf{errori casuali} (o incertezze) e \textbf{errori sistematici}. Gli errori sistematici sono dovuti a effetti trascurati dall'esperimento, come errori teorici o di calibrazione strumentale, e possono essere generalmente eliminati a priori o a posteriori. Le incertezze sono inevitabili e sono l'oggetto di studio della statistica e dell'analisi degli errori.

\subsection{Rappresentazione delle incertezze}
Quando si presenta il risultato di una misurazione o di un esperimento è fondamentale presentare anche l'errore. A causa dell'inevitabilità delle incertezze è impossibile misurare il valore vero di una grandezza e dobbiamo quindi accontentarci di presentare un intervallo all'interno del quale è probabile ricada.\\
$\Rightarrow$ La misurazione va presentata nella forma: $x_{best} \pm \delta x$, con $\delta x > 0$\\
$\Rightarrow$ Il valore vero ricadrà quindi nell'intervallo $[x_{best} - \delta x, x_{best} + \delta x]$

\subsection{Cifre significative}
Le \textbf{cifre significative} di una misura sono le cifre conosciute con ragionevole certezza. Quando si presenta un risultato è fondamentale tenerne conto e seguire alcune regole:
\begin{itemize}
    \item Le incertezze sono solitamente arrotondate alla prima cifra significativa, a meno che questa non sia un 1 o in alcuni casi un 2
    \item L'ultima cifra significativa in una misura dovrebbe essere dello stesso ordine di grandezza dell'incertezza
    \item Una cifra che rappresenta una piccola frazione dell'errore non è significativa
\end{itemize}
Un'incertezza può anche essere rappresentata in forma di \textbf{incertezza relativa}: $\delta x_{rel} = \frac{\delta x}{|x|}$\\
Questa grandezza sarà adimensionale (l'incertezza "assoluta" ha la stessa unità di misura della grandezza) e ci permette di meglio interpretare la grandezza dell'errore compiuto durante la misurazione. Inoltre è strettamente legata al concetto di cifra significativa: una grandezza con N cifre significative avrà un errore di circa 1 sulla $N_{esima}$ cifra.

\subsection{Caratteristiche di uno strumento}
Uno strumento è caratterizzato da
\begin{itemize}
    \item \textbf{Accuratezza:}
    Uno strumento è accurato se la sua misura è vicina al valore reale.
    \item \textbf{Precisione:}
    Uno strumento è preciso se le sue misure sono vicine fra loro.
    \item \textbf{Sensibilità:}
    Rappresenta il minimo valore che lo strumento può misurare.
    \item \textbf{Risoluzione:}
    Rappresenta la minima distanza tra due misure affinchè lo strumento le possa distinguere.
    \item \textbf{Portata:}
    Rappresenta il massimo valore che lo strumento può misurare.
\end{itemize}
Gli errori sistematici influenzano l'accuratezza, mentre quelli casuali la precisione. [\textit{Todo: Immagine}]

\subsection{Incertezza su misure ripetute}
Ripetere più volte una misurazione permette di ridurre l'incertezza dovuta a errori casuali. In particolare la migliore stima del valore reale nella maggior parte dei casi è la \textbf{media}:
$\bar{x} = \frac{\sum_{i=1} ^N x_i}{N}$\\\\
Se gli errori sono veramente casuali la loro media risulterà 0, quindi si introduce una grandezza nota come \textbf{varianza} o \textbf{scarto quadratico medio}: $\sigma_x ^2 = \frac{\sum^N _{i=1} (x_i - \mu)^2}{N}$
dove $\mu$ è il valore vero.\\ \\
Quando $\mu$ non è noto e non è possibile stimarlo accuratamente si utilizza la \textbf{varianza campionaria}: $S^2 _x = \frac{\sum^N _{i=1} (x_i - \bar{x})^2}{N - 1}$\\ \\
Per avere le stesse unità di misura del valore misurato introduciamo la \textbf{deviazione standard}:
$\sigma_x = \sqrt{\sigma^2 _x} = \sqrt{\frac{\sum^N _{i=1} (x_i - \mu)^2}{N}}$ \\\\ e \textbf{deviazione standard dalla media}: $S_x = \sqrt{S^2 _x} = \sqrt{\frac{\sum^N _{i=1} (x_i - \bar{x})^2}{N - 1}}$ \\\\
Il termine N-1 a denominatore compare come correzione per il vincolo introdotto usando i valori per calcolare la media, che è parametro della varianza campionaria, secondo un principio che verrà spiegato in seguito.\\\\
All'aumentare del numero di misurazioni, il valore medio sarà sempre più vicino a quello reale, per questo si introduce la \textbf{deviazione standard della media}:
$S_{\bar{x}}= \frac{S_x}{\sqrt{N}}$\\\\
La deviazione standard può essere usata per esprimere l'errore sulla singola misura, mentre la variazione standard della media per l'errore del valore medio.

\subsection{Propagazione delle incertezze}
Spesso capita che le grandezze non possano essere misurate direttamente, ma può essere necessario calcolarle a partire da altre misurazioni. I risultati saranno anch'essi affetti da errori che devono essere calcolati applicando apposite formule.\\\\
Sia $f(x_1, x_2, ..., x_N)$ una funzione qualsiasi di variabili \textbf{indipendenti} e $y=f(x_1, x_2, ..., x_n)$ la variabile sulla quale vogliamo calcolare l'errore, allora: 
$\delta y = \sqrt{\sum_{i=1} ^N \left(\frac{\partial f}{\partial x_i} \delta x_i \right)^2}$\\\\
Nel caso in cui le variabili non fossero indipendenti dovremmo considerare anche i \textbf{coefficienti di covarianza}:\\ $\delta y = \sqrt{\sum_{i=1} ^N \left(\frac{\partial f}{\partial x_i} \delta x_i \right)^2 + 2 \cdot \frac{\partial f}{\partial x_1} \frac{\partial f}{\partial x_2} \cdot \delta_{x1 x2} + ... + 2 \cdot \frac{\partial f}{\partial x_{N-1}} \frac{\partial f}{\partial x_N} \cdot \delta_{xN-1 xN}}$\\\\
In molti dei casi più comuni è possibile applicare regole più semplici ricavabili da quella generale. Sarà poi possibile applicare una combinazione delle regole specifiche per calcolare errori anche di funzioni complicate. Queste regole sono valide solo per variabili indipendenti.
\begin{itemize}
    \item \textbf{Somma e differenza}:\\
    Sia $q=x_1 \pm x_2 \pm ... \pm x_N$ allora $\delta q = \sqrt{\left( \delta x_1 \right)^2 + \left( \delta x_2 \right)^2 + ... + \left( \delta x_N \right)^2}$

    \item \textbf{Prodotto e rapporto}:\\
    Sia $q = \frac{x_1 \times ... \times x_N}{y_1 \times ... \times y_N}$ allora $\frac{\delta q}{|q|} = 
    \sqrt{\left( \frac{\delta x_1}{x_1} \right)^2 + ... + \left( \frac{\delta x_N}{x_N} \right)^2 + \left( \frac{\delta y_1}{y_1} \right)^2 + ... + \left( \frac{\delta y_N}{y_N} \right)^2}$

    \item \textbf{Potenza}:\\
    Sia $q=x^n$ allora $\frac{\delta q}{|q|} =|n| \cdot \frac{\delta x}{|x|}$
    
    \item \textbf{Funzione in una sola variabile}:\\
    Sia $q=f(x)$ allora $\delta q = \left| \frac{df}{dx} \right| \cdot \delta x$
    
\end{itemize}
Nel caso in cui la correlazione non sia trascurabile è possibile calcolare i \textbf{coefficienti di covarianza} come:\\
$\sigma_{xy} = \frac{1}{N} \sum(x_i - \bar{x})(y_1 - \bar{y})$\\\\
Questi valori oltre a darci un'indicazione di quanto l'errore sulla prima variabile sia legato all'errore sulla seconda, ci permettono di verificare se i valori sono legati da una relazione del tipo $y = A + Bx$. Per fare ciò possiamo calcolare il \textbf{coefficiente di correlazione lineare}: $r = \frac{\sigma_{xy}}{\sigma_x \sigma_y}$\\\\
Il coefficiente sarà sempre compreso tra -1 e 1. Un valore pari a 0 indica che non vi è nessuna correlazione, mentre un valore di $\pm1$ indica una correlazione perfetta. Per prendere decisioni riguardo alla presenza o meno di una relazione lineare tra due variabili è necessario consultare apposite tabelle che associano ai valori di r la probabilità che esista o meno questo legame.

\subsection{Medie pesate}
Nel caso si abbiano diverse misure indipendenti della stessa grandezza con incertezza differente, la miglior stima del valore reale non è più la semplice media dei valori.\\
Per tenere conto delle diverse deviazioni si introduce la \textbf{media pesata}: $x_{wav} = \frac{\sum w_i x_i}{\sum w_i}$ con $w_i = \frac{1}{\sigma_i ^2}$\\\\
\textbf{L'incertezza} su questo valore si ricava applicando la propagazione degli errori: $\sigma_{wav} = \frac{1}{\sqrt{\sum w_i}}$

\subsection{Criterio di Chauvenet per il rigetto dei dati}
Eseguendo ripetutamente un esperimento è possibile ottenere una o più misure che si discostano di molto dalla media. Queste possono essere dovute a errori sperimentali, al caso o ad effetti fisici ignorati nell'esperimento. Considerare o scartare queste misure è una questione molto controversa.\\\\
Un possibile criterio per prendere questa decisione è il criterio di Chauvenet. Per prima cosa calcoliamo media e variazione standard considerando tutti i dati, poi calcoliamo il numero di variazioni standard di distanza del valore sospetto dalla media $t_{sos}=\frac{|s_{sos}-\bar{x}}{\sigma_x}$. Da questo valore usando apposite tabelle è possibile ricavare la probabilità $P(t_{sos}\sigma)$ di ottenere quella misura conoscendone la distribuzione. A questo punto calcoliamo quanti valori con distanza simile dovremmo aspettarci in base al numero di misure: $n=N\times P(t_{sos}\sigma)$. Per valori di $n$ abbastanza piccoli è possibile rigettare il valore, il criterio di Chauvenet prevede il rigetto per $n<\frac{1}{2}$, ma è possibile usare valori diversi.

\subsection{Test di Fisher (ANOVA)}
Il test di Fisher permette di verificare se due o più campioni provengono da una stessa popolazione, partendo dalle varianze.\\\\
Definiamo la \textbf{varianza entro i gruppi} come la media delle varianze dei singoli campioni: $\sigma^2_{entro}=\frac{\sum\sigma_i^2}{a}$, con $a$ = al numero di gruppi.\\
La \textbf{varianza tra i gruppi} si definisce come la varianza dei valori medi per il numero di elementi: $\sigma^2_{tra}=\frac{\sum(\bar{x}_i-\bar{x})^2}{a-1}\times N$ con $\bar{x}=\frac{\sum\bar{x}_1}{a}$\\
A questo punto è possibile confrontare la variabile $F=\frac{\sigma^2_{tra}}{\sigma^2_{entro}}$ con i valori tabulati per ottenere una probabilità. Se i campioni appartenessero alla stessa popolazione e in assenza di fluttuazioni statistiche $F=1$.

\section{Le distribuzioni}
Dopo aver ripetuto più volte una misurazione e aver ottenuto un certo numero di valori rimane il problema di come presentarli. L'elenco dei valori di per se non fornisce informazioni utili e il valore medio con la sua incertezza può a volte non essere sufficiente, in quanto possiamo ricavare informazioni utili dalla distribuzione.

\subsection{Istogrammi}
La prima cosa da fare per presentare dei dati e ordinarli in ordine crescente. Dopodichè possiamo raggruppare i valori uguali e contare quante volte si ripetono, questo ci permette di calcolare la \textbf{frequenza} F di ogni valore $x_k$: $F=\frac{n_k}{N}$\\\\
A questo punto è possibile riscrivere la \textbf{media} come: $\bar{x} = \sum_k x_k F_k$\\\\
Se in un grafico rappresentiamo in ascissa i valori e in ordinata la loro frequenza otteniamo un \textbf{istogramma a barre}.\\
Nel caso in cui una variabile assuma valori continui un istogramma a barre può risultare poco significativo in quanto ogni valore comparirà probabilmente una singola volta. Per risolvere il problema possiamo raggruppare i valori in \textbf{intervalli} o bin e ottenere un \textbf{istogramma a intervalli}.

\subsection{Distribuzioni limite}
All'aumentare del numero di misurazioni l'istogramma si avvicinerà sempre di più a una curva continua. Se noi poniamo $N=+ \infty$ otterremo una curva f(x). Questa curva ha alcune proprietà interessanti.\\\\
Il \textbf{numero di valori} che ricadono tra x=a e x=b sarà dato da: $P(a, b) = \int_a ^b f(x) dx$ \quad e $\int_{- \infty} ^ {+ \infty} f(x)dx = 1$ \\\\
La \textbf{media} sarà uguale a: $\bar{x} = \int_{- \infty} ^ {+ \infty} x f(x) dx$\\\\
La \textbf{deviazione standard}: $\sigma_x ^2 = \int_{- \infty} ^ {+ \infty} \left( x - \bar{x} \right)^2 f(x)dx$\\

\subsection{La distribuzione di Gauss}
Disegnando le distribuzioni limite per molte grandezze affette da errori casuali ci accorgeremmo che queste hanno quasi sempre una tipica \textbf{forma a campana} incentrata sul valore medio/valore reale. In effetti molte misure affette da incertezze possono essere rappresentate accuratamente da una curva nota come \textbf{distribuzione di Gauss} o \textbf{distribuzione normale} che ha questa tipica forma.\\\\
La distribuzione di Gauss ha equazione:
$G_{x,\sigma}(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^\frac{-(x - X)^2}{2 \sigma ^2}$\\\\
Il parametro $\sigma$ è la \textbf{deviazione standard} dei valori e X è il \textbf{valore reale}.\\\\
La deviazione standard influenza la larghezza della curva e l'altezza del picco.\\\\
Come spiegato in precedenza vale la relazione $P(a, b) = \int _{X - a} ^{X + a} G_{x,\sigma} (x)dx$ e possiamo quindi calcolare che la percentuale di valori che ricadono entro $\pm \sigma$ da X è circa pari al 68\%.

\subsection{La distribuzione binomiale}
La distribuzione binomiale descrive la probabilità che un evento aleatorio con probabilità nota avvenga un certo numero di volte. Ci permette di modellare e studiare molti esperimenti che non potrebbero facilmente essere descritti con la distribuzione gaussiana.\\\\
La distribuzione ha forma:
$B_{n,p}(\nu) = \binom{n}{\nu} \cdot p^\nu q^{n - \nu}$, dove p rappreseta la \textbf{probabilità del singolo evento}, n il \textbf{numero totale di eventi} e $\nu$ il \textbf{numero di successi}.\\\\
Il \textbf{valore medio} risulta: $\bar{v} = np$\\\\
E la \textbf{deviazione standard}: $\sigma_v = \sqrt{np(1-p)}$\\\\
La distribuzione è discreta e asimmetrica (tranne nel caso $p=\frac{1}{2}$), ma per n abbastanza grande può essere approssimata da una gaussiana con $X=np$ e $\sigma = \sqrt{np(1-p)}$

\subsection{La distribuzione di Poisson}
La distribuzione di Poisson descrive esperimenti di conteggio di eventi casuali, ma con \textbf{tasso medio definito}. Si tratta di una semplificazione della binomiale nel caso di un numero di eventi molto grande e una probabilità molto
piccola.\\\\
La distribuzione ha forma: $P_\mu (\nu) = e^{-\mu} \frac{\mu^{\nu}}{\nu!}$, dove $\mu$ rappresenta il \textbf{numero medio di conteggi} ottenuti se ripetiamo molte volte la misura.\\\\
Se denotiamo con R il tasso medio con cui si verificano gli eventi e con T un intervallo di tempo allora: $\mu = RT$\\\\
Il \textbf{valore medio} è $\mu$ e la \textbf{deviazione standard}: $\sigma_v = \sqrt{\mu}$. Anche questa distribuzione per $\mu$ sufficientemente grande può essere approssimata da una gaussiana, con $X=\mu$ e $\sigma = \sqrt{\mu}$ e quindi definita da un solo parametro.

\subsection{La distribuzione di Student}
Quando i campioni da analizzare sono piccoli le stime dei parametri della distribuzione risultano particolarmente incerte. In questo caso è possibile utilizzare la distribuzione di Student che applica alla variabile t di Student: $t=\frac{\bar{x}-\mu}{S_x/\sqrt{N}}$ dove $\frac{S_x}{\sqrt{N}} = S_{\bar{x}}$ è la variazione standard della media.\\\\
La variabile $t$ si distribuisce secondo la funzione: $f(t)=C\cdot \left(1+\frac{t^2}{\nu}\right)^{\frac{-(\nu+1)}{2}}$, dove il parametro C si ricava dalla condizione di normalizzazione $\int_{-\infty}^{+\infty}f(t) = 1$ e $\nu(N)=N-1$ è il numero di gradi di libertà.\\\\
La deviazione standard è $\sigma = \frac{N}{N-2}$.\\\\
La variabile t mette in confronto lo scarto della media rispetto al valore reale e la sua variazione standard e ci permette quindi di ricavare informazioni importanti sul livello di confidenza di un intervallo di valori. Se denotiamo con $t_{0.95}$ l'estremo superiore di un intervallo nel quale siamo sicuri con livello di confidenza $0.95=95\%$ si trovi il valore allora: $\int_{-t_{0.975}}^{+t_{0.975}}f(t)=95\%$ \quad e $\int_{-\infty}^{-t_{0.975}}f(t)=\int_{+t_{0.975}}^{+\infty}f(t)=2.5\%$\\\\
I valori degli estremi in funzione dei livelli di confidenza e dei parametri della distribuzione sono tabulati e ci permettono di applicare un importante test statistico per confrontare la compatibilità di più misurazioni.\\\\
Il numeratore della variabile di Student può essere sostituito dalla differenza tra variabili accoppiate e il denominatore dalla relativa variazione standard in modo da poter valutare la compatibilità delle due.\\\\
La distribuzione di Student ha come unico parametro $\nu(N)$, per un valore di $N$ abbastanza grande ($N>30$ indicativamente) diventa indistinguibile da una gaussiana.

\subsection{Metodo dei minimi quadrati}
In diversi esperimenti può essere utile cercare la \textbf{relazione matematica} che lega tra loro diverse variabili. A volte questa legge può risultare evidente leggendo un grafico, ma non risulta comunque semplice ricavare i parametri della curva che approssima la distribuzione,in quanto le grandezze sono sempre affette da errori. Considerando valori distribuiti normalmente attorno alla curva è possibile ricavare i parametri applicando il \textbf{principio della massima verosimiglianza}, massimizzando cioè la funzione che rappresenta la probabilità di ottenere un certo valore da una misurazione (=l'equazione della distribuzione).\\\\
Nel caso di una retta del tipo $y = A + Bx$ e assumendo che l'incertezza su x sia piccola questo procedimento fornisce come \textbf{stima di A e B}: \\
$A = \frac{\sum x2 \sum y - \sum x \sum xy}{\Delta}$ \qquad $B = \frac{N\sum xy - \sum x \sum y}{\Delta}$ \qquad con $\Delta = N \sum x^2 - \left(\sum x \right)^2$\\\\
La variabile y calcolata dalla funzione e i parametri della curva sono tutti affetti da \textbf{incertezza}. In particolare: \\
$\sigma_y = \sqrt{\frac{1}{N-2} \sum (y_i - A - Bx_i)^2}$ \qquad
$\sigma_A = \sigma_y \sqrt{\frac{\sum x^2}{\Delta}}$ \qquad
$\sigma_B = \sigma_y \sqrt{\frac{N}{\Delta}}$\\\\
Nel caso in cui x sia affetto da incertezza e y no sarà sufficiente invertire il ruolo delle variabili.\\\\
Un'incertezza su x può essere rappresentata con un'incertezza corrispondente in y: $\sigma_y(equiv) = \frac{dy}{dx}\sigma_x$\\
Così se entrambe le variabili sono affette da errore il metodo è comunque applicabile, ma l'incertezza diventa:\\ $\sigma_y(equiv) = \sqrt{\sigma_y ^2 + \left(\frac{dy}{dx}\right)^2}$\\\\
Questo procedimento è valido anche per altre curve. Massimizzare la funzione probabilità (e minimizzare quindi l'esponente) può fornire equazioni e sistemi per il calcolo dei parametri difficilmente risolvibili, in alcuni casi impossibili. Un'altra strada consiste nel riportare la curva a una retta applicando dei cambi di variabile e successivamente propagare l'errore, ma questo non è sempre possibile.\\\\
Se le misure sono note con deviazioni differenti è possibile applicare dei pesi nel calcolo dei parametri, con lo steso metodo applicato nelle medie pesate.


\subsection{Test del $\chi^2$ per le distribuzioni}
Per verificare che una serie di misurazioni sia ragionevolmente compatibile con una distribuzione nota possiamo utilizzare il \textbf{metodo del} $\chi^2$. Per prima cosa raggruppiamo i valori in intervalli. Il numero di intervalli dovrà essere scelto in base alle caratteristiche dei dati e della distribuzione che vogliamo confrontare. Ogni intervallo dovrebbe contenere almeno circa 5 o più valori e gli intervalli devono essere in numero sufficientemente grande.\\\\
Se denotiamo con \textbf{$O_k$} il \textbf{numero di valori osservato} nell'intervallo $K_{esimo}$ e con \textbf{$E_k$} il \textbf{numero di valori previsto} dalla distribuzione nell'intervallo allora: $\chi^2 = \sum_{K=1} ^n \left(\frac{O_k - E_k}{\sigma_k} \right)^2$\\  
Nel caso gli errori siano piccoli e casuali (come in una distribuzione gaussiana): $\chi^2 = \sum_{K=1} ^n \left(\frac{O_k - E_k}{\sqrt{E_k}} \right)^2$\\\\
Dobbiamo ora definire il concetto di \textbf{gradi di libertà} già precedentemente citato. In un calcolo statistico il numero di gradi di libertà è dato dal numero di valori osservati meno il numero di parametri da essi calcolato: $d = N - c$, con N uguale al numero di dati e c al numero di vincoli.\\\\
Il valore di $\chi^2$ atteso è uguale al numero di gradi di libertà, si introduce quindi una nuova grandezza che permette di meglio confrontare queste due grandezze, il \textbf{$\chi^2$ ridotto}: $\Tilde{\chi}^2 = \frac{\chi^2}{d}$\\\\
Questo valore dovrebbe essere vicino a 1 se i dati sono realmente distribuiti secondo la distribuzione da confrontare, ma per sapere se un valore ottenuto è accettabile o meno è necessario calcolare la probabilità di ottenerlo assumendo che la distribuzione sia corretta. I valori delle probabilità sono difficili da calcolare, ma si trovano riportati in apposite tabelle.\\\\
Quando si applica questo test statistico è opportuno riportare la percentuale insieme alla nostra conclusione, perché la decisione finale è comunque rimandata al buonsenso del singolo. Nonostante ciò tradizionalmente esistono dei \textbf{valori limite} al di sotto dei quali la compatibilità è esclusa: il 5\% e l'1\%.

\section{Probabilità}
Nel corso degli anni sono state date diverse definizioni di probabilità. Le principali sono le definizioni a priori e posteriori e quella assiomatica moderna.
\subsection{Probabilità a priori e a posteriori}
Le prime definizioni utilizzate sono molto intuitive, ma poco rigorose. Conoscendo le caratteristiche del sistema è possibile definire la probabilità a priori come $P(E)=\frac{n}{N}$, dove $N$ rappresenta il numero di casi possibili, $n$ il numero di casi favorevoli e $E$ l'evento. Questa definizione è applicabile solo se gli $N$ eventi possibili sono tutti equiprobabili.\\\\
Nel caso più comune non si conoscono a priori le caratteristiche di un sistema o non è possibile contare facilmente tutti i casi. Per risolvere questo problema si può provare a definire la probabilità a posteriori, con la stessa equazione $P(E)=\frac{n}{N}$, ma dove i parametri $N$ e $n$ rappresentano i casi misurati invece di quelli calcolati in precedenza. Per $n \rightarrow +\infty$, cioè per un numero molto grande di misurazioni la probabilità a posteriori si avvicinerà e stabilizzerà sempre di più intorno al valore reale.
\subsection{La legge dei grandi numeri}
Non è possibile utilizzare direttamente il concetto di limite applicandolo alla probabilità perché nonostante essa si avvicini sempre di più al valore reale, continua ad oscillare e non vi tende quindi secondo la definizione matematica. Esiste però una legge che descrive questo comportamento ed è nota come legge dei grandi numeri.\\\\
Scelto un $\epsilon>0$ qualsiasi, il limite della probabilità che la differenza tra la frequenza misurata ($\frac{n}{N}$) e la probabilità reale di un evento sia minore di $\epsilon$ tende a 1: \quad
$lim_{N \rightarrow +\infty} P^*\{\left|\frac{n}{N} - P \right| < \epsilon \} = 1$
\subsection{Probabilità assiomatica}
La definizione moderna di probabilità si basa su una serie di assiomi che ne danno una descrizione senza però descrivere come calcolarla.
\begin{itemize}
    \item \textbf{Assioma della positività}\\
    La probabilità associata a un evento è sempre positiva
    \item \textbf{Assioma della certezza}\\
    La probabilità associata a un evento certo è sempre 1
    \item \textbf{Assioma dell'unione}\\
    Se due eventi sono tra loro esclusivi, la probabilità che avvenga uno oppure l'altro è la somma delle probabilità associate ai due
\end{itemize}
Da questi 3 assiomi è possibile ricavare una serie di teoremi che forniscono strumenti utili per il calcolo.
\begin{itemize}
    \item $P(\emptyset)=0$
    \item $\forall E$ $0 \leq P(E) \leq 1$
    \item Se $\bar{E}$ è l'evento del non verificarsi di $E$, allora $P(E) + P(\bar{E}) = 1$
    \item Se $E_1, E_2$ non sono mutualmente esclusivi, $E_1 \bigcap E_2 \neq \emptyset$, allora $P(E_1 \bigcup E_2)=P(E_1)+P(E_2)-P(E_1 \bigcap E_2)$
    \item La probabilità condizionata $P(E_1|E_2)$, cioè la probabilità che si verifichi $E_2$ verificatosi $E_1$ se questi non sono indipendenti è uguale a: $P(E_1|E_2)=\frac{P(E_1 \bigcup E_2)}{P(E_1)}$
    \item La probabilità composta, cioè la probabilità che due eventi si verifichino insieme se $E_1$ e $E_2$ non sono indipendenti: $P(E_1 \bigcap E_2)=P(E_1) \cdot P(E_2|E_1)$, se sono indipendenti: $P(E_1 \bigcap E_2) =P(E_1) \cdot P(E_2)$
\end{itemize}



\newpage
\renewcommand*\contentsname{Indice}
\tableofcontents


\end{document}
